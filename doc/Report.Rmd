<link rel="stylesheet" href="http://netdna.bootstrapcdn.com/bootstrap/3.0.3/css/bootstrap.min.css">
<style type="text/css"> body {padding: 10px 30px 10px 30px;} table,th, td {text-align: center;} </style>


Data Analytics for Business - GROUP CASE
========================================================

**FMCG Case - South-East Asia, INSEAD**

Ankit Mathur<BR>
Rob Wegbrands<BR>
Ankiit Miittal<BR>
Anuradha Vasudeva<BR>
Antoine Paris

<img src="https://ldetrainees.files.wordpress.com/2014/05/using-big-data.jpg">

Introduction
---------------------------------------------------------

### The Context

In FMCG retail Industry, one of the key challenge is to **forecast sales across the year** in order to have products available in stores. This ensures to meet customer needs not only in term of quantity but also in term of timing: there is **no value created if products seat on shelves** or in the stock room for weeks before being actually sold. Moreover, there is a **risk of loss for perished goods**. This is also the challenge faced when introducing new products in the range, and we do not - usually - have historical sales to forecast order and sales across the year.

Major FMCG retailers usually sell around **50,000 skus (Stock Keeping UnitS)** and keep around **45 days average stock** across distribution centers and stores (high rotation items have few days stock while some other items actually gets more than 3 months of stock). As a matter of fact, improper forecast will have important negative financial consequences or even damage the brand image. 

On the one hand, **excessive order will generate overstock** in stores generating excess inventory and increasing working capital requirement. Later on, excess inventory would require to operate some **adhoc promotion** to deplete stock - shrinking margins which are already low in retail Business - while sending **"wrong messages???** to customers: promotions primarily aim to generate desire at customer???s level to buy a product with a precise and coherent long term strategy. It is not supposed to compensate for a poor inventory management and regulate the stock level.
On the other hand, out of stock products will have serious consequences on the Business: with empty shelves, stores will not only lose sales opportunity but also may **lose loyal customers**, which mean on the long run market share. After all, the only thing that is more annoying than shopping in an hypermarket is shopping in an hypermarket and find empty shelves. 

Moreover, sales forecast has always been a **source of internal conflicts** in every retail organization: Buyers and merchandisers are usually incentivized on sales target and tend to plan on **over-optimistic** or even **excessive order**. On the other hand, Supply Chain and Finance, respectively worrying about excess of physical inventory or increase in working capital, tend to be **too conservative** and put pressure to lower order forecast. Ordering and replenishment team - usually coordinating between both teams - need to deal with this situation and has to manage the trade off between keeping low inventory while securing sales for the sake of the Business.

Our Approach Part #1
---------------------------------------------------------------
We used actual data (modified for confidentiality reason) showing consolidated retail stores chain sales per product category per month.

The raw data basically shows (1) 12 columns for monthly sales [january to december] and (2) 565 categories of class of products (Rice, Oil, coffee, Beer, TV, paper, etc???)


```{r echo=FALSE, message=FALSE, prompt=FALSE, results='asis'}
# let's make the data into data.matrix classes so that we can easier visualize them
ProjectDataFactor = data.matrix(ProjectDataFactor)
ProjectData = data.matrix(ProjectData)
```

This is how the first `r min(max_data_report, nrow(ProjectData))` data looks:
<br>

<div class="row">
<div class="col-md-6">
```{r echo=FALSE, message=FALSE, prompt=FALSE, results='asis'}
show_data = data.frame(round(ProjectDataFactor,2))
show_data = show_data[1:min(max_data_report,nrow(show_data)),]
row<-rownames(show_data)
dfnew<-cbind(row,show_data)
change<-colnames(dfnew)
change[1]<-"Variables"
colnames (dfnew)<-change
m1<-gvisTable(dfnew,options=list(showRowNumber=TRUE,width=1220, height=min(400,27*(nrow(show_data)+1)),allowHTML=TRUE,page='disable'))
print(m1,'chart')
```
</div>
</div>
<br> <br>

#### Step 1: Confirm the data are metric

The data we use here have the following descriptive statistics: 

<div class="row">
<div class="col-md-6">
```{r echo=FALSE, comment=NA, warning=FALSE, message=FALSE,results='asis'}
show_data = data.frame(round(my_summary(ProjectDataFactor),2))
#show_data = show_data[1:min(max_data_report,nrow(show_data)),]
row<-rownames(show_data)
dfnew<-cbind(row,show_data)
change<-colnames(dfnew)
change[1]<-"Variables"
colnames (dfnew)<-change
m1<-gvisTable(dfnew,options=list(showRowNumber=TRUE,width=1220, height=min(400,27*(nrow(show_data)+1)),allowHTML=TRUE,page='disable'))
print(m1,'chart')
```
</div>
</div>


```{r, results='asis'}
ProjectDatafactor_scaled=apply(ProjectDataFactor,2, function(r) {if (sd(r)!=0) res=(r-mean(r))/sd(r) else res=0*r; res})
```

#### Step 2: Decide whether to scale or standardize the data

Notice now the summary statistics of the scaled dataset:

<br>


<div class="row">
<div class="col-md-6">
```{r echo=FALSE, comment=NA, warning=FALSE, message=FALSE,results='asis'}

show_data = data.frame(round(my_summary(ProjectDatafactor_scaled),2))
#show_data = show_data[1:min(max_data_report,nrow(show_data)),]
row<-rownames(show_data)
dfnew<-cbind(row,show_data)
change<-colnames(dfnew)
change[1]<-"Variables"
colnames (dfnew)<-change
m1<-gvisTable(dfnew,options=list(showRowNumber=TRUE,width=1220, height=min(400,27*(nrow(show_data)+1)),allowHTML=TRUE,page='disable'))
print(m1,'chart')
```
</div>
</div>

<br>
As expected all variables have mean 0 and standard deviation 1. 

#### Step 3:  Check correlation matrix to see if Factor Analysis makes sense

This is the correlation matrix of the `r length(factor_attributes_used)` original variable we use for factor analysis:

```{r echo=FALSE, comment=NA, warning=FALSE, message=FALSE, results='asis'}
thecor = round(cor(ProjectDataFactor),2)
colnames(thecor)<-colnames(ProjectDataFactor)
rownames(thecor)<-colnames(ProjectDataFactor)
## printing the result in a clean-slate table
#cat(renderHeatmapX(thecor, border=1,center = 0,vrange_up = 1, vrange_down = 1))
cat(renderHeatmapX(thecor, border=1))
    
```

Let's now see what factor analysis suggests as factors. 

#### Step 4. Develop a scree plot and decide on the number of factors to be derived

We decided to apply the segmentation methodology to combine some months and highlight some 'season trend' across the year. Obviously there are 12 months in the year but some months show similar sales from one to another. 

```{r echo=FALSE, comment=NA, warning=FALSE, message=FALSE,results='asis'}
UnRotated_Results<-principal(ProjectDataFactor, nfactors=ncol(ProjectDataFactor), rotate="none",score=TRUE)
UnRotated_Factors<-round(UnRotated_Results$loadings,2)
UnRotated_Factors<-as.data.frame(unclass(UnRotated_Factors))
colnames(UnRotated_Factors)<-paste("Component",1:ncol(UnRotated_Factors),sep=" ")
```

Each factor has an eigenvalue as well as the percentage of the variance explained. The sum of the eigenvalues of the components is equal to the number of original raw attributes used for factor analysis, while the sum of the percentages of the variance explained across all components is 100%. For example, for our data these are:

<br>

```{r echo=FALSE, comment=NA, warning=FALSE, error=FALSE,message=FALSE,results='asis'}
Variance_Explained_Table_results<-PCA(ProjectDataFactor, graph=FALSE)
Variance_Explained_Table<-Variance_Explained_Table_results$eig
Variance_Explained_Table_copy<-Variance_Explained_Table


row=1:nrow(Variance_Explained_Table)
name<-paste("Component No:",row,sep="")
Variance_Explained_Table<-cbind(name,Variance_Explained_Table)
Variance_Explained_Table<-as.data.frame(Variance_Explained_Table)
colnames(Variance_Explained_Table)<-c("Components", "Eigenvalue", "Percentage_of_explained_variance", "Cumulative_percentage_of_explained_variance")

m<-gvisTable(Variance_Explained_Table,options=list(width=1200, height=min(400,27*(nrow(show_data)+1)),allowHTML=TRUE,page='disable'),formats=list(Eigenvalue="#.##",Percentage_of_explained_variance="#.##",Cumulative_percentage_of_explained_variance="#.##"))
print(m,'chart')
```


One can also plot the eigenvalues of the generated factors in decreasing order: this plot is called the **scree plot**. For our data this plot looks as follows:


```{r Fig1, echo=FALSE, comment=NA, results='asis', message=FALSE, fig.align='center', fig=TRUE}

eigenvalues  <- Variance_Explained_Table[,2]
df           <- cbind(as.data.frame(eigenvalues), c(1:length(eigenvalues)), rep(1, length(eigenvalues)))
colnames(df) <- c("eigenvalues", "components", "abline")
Line         <- gvisLineChart(as.data.frame(df), xvar="components", yvar=c("eigenvalues","abline"), options=list(title='Scree plot', legend="right", width=900, height=600, hAxis="{title:'Number of Components', titleTextStyle:{color:'black'}}", vAxes="[{title:'Eigenvalues'}]",  series="[{color:'green',pointSize:3, targetAxisIndex: 0}]"))
print(Line, 'chart')
```



```{r echo=FALSE, comment=NA, warning=FALSE,message=FALSE,results='asis'}
if (factor_selectionciterion == "eigenvalue")
  factors_selected = sum(Variance_Explained_Table_copy[,1] >= 1)
if (factor_selectionciterion == "variance")
  factors_selected = 1:head(which(Variance_Explained_Table_copy[,"cumulative percentage of variance"]>= minimum_variance_explained),1)
if (factor_selectionciterion == "manual")
  factors_selected = manual_numb_factors_used

```


#### 5. Interpret the factors
 
For our data, the `r factors_selected` selected factors look as follows after the `r rotation_used` rotation: 

```{r echo=FALSE, comment=NA, warning=FALSE, message=FALSE,results='asis'}

Rotated_Results<-principal(ProjectDataFactor, nfactors=max(factors_selected), rotate=rotation_used,score=TRUE)
Rotated_Factors<-round(Rotated_Results$loadings,2)
Rotated_Factors<-as.data.frame(unclass(Rotated_Factors))
colnames(Rotated_Factors)<-paste("Component",1:ncol(Rotated_Factors),sep=" ")

sorted_rows <- sort(Rotated_Factors[,1], decreasing = TRUE, index.return = TRUE)$ix
Rotated_Factors <- Rotated_Factors[sorted_rows,]

show_data <- Rotated_Factors 
#show_data = show_data[1:min(max_data_report,nrow(show_data)),]
row<-rownames(show_data)
dfnew<-cbind(row,show_data)
change<-colnames(dfnew)
change[1]<-"Variables"
colnames (dfnew)<-change
m1<-gvisTable(dfnew,options=list(showRowNumber=TRUE,width=1220, height=min(400,27*(nrow(show_data)+1)),allowHTML=TRUE,page='disable'))
print(m1,'chart')
```


To better visualize and interpret the factors we "supress" loadings with small values, e.g. with absolute values smaller than `r MIN_VALUE`. In this case our factors look as follows after suppressing the small numbers:

```{r echo=FALSE, comment=NA, warning=FALSE, message=FALSE,results='asis'}
Rotated_Factors_thres <- Rotated_Factors
Rotated_Factors_thres[abs(Rotated_Factors_thres) < MIN_VALUE]<-NA
colnames(Rotated_Factors_thres)<- colnames(Rotated_Factors)
rownames(Rotated_Factors_thres)<- rownames(Rotated_Factors)

show_data <- Rotated_Factors_thres 
#show_data = show_data[1:min(max_data_report,nrow(show_data)),]
row<-rownames(show_data)
dfnew<-cbind(row,show_data)
change<-colnames(dfnew)
change[1]<-"Variables"
colnames (dfnew)<-change
m1<-gvisTable(dfnew,options=list(showRowNumber=TRUE,width=1220, height=min(400,27*(nrow(show_data)+1)),allowHTML=TRUE,page='disable'))
print(m1,'chart')
```


**By applying segmentation process, we achieved to 'group some months'  and reduce timeline to 4 segment: **

**Peak season (december), high season (october), normal season (april) and low season (july).**


#### Step 6. Save factor scores for subsequent analyses

For our data, using the rotated factors we selected, we can create a new dataset where our observations are as follows :

<div class="row">
<div class="col-md-6">

```{r echo=FALSE, message=FALSE, prompt=FALSE, results='asis'}
NEW_ProjectData <- round(Rotated_Results$scores[,1:factors_selected,drop=F],2)
colnames(NEW_ProjectData)<-paste("Derived Variable (Factor)",1:ncol(NEW_ProjectData),sep=" ")

if (factors_selected >=2){ 
  
  show_data <- as.data.frame(NEW_ProjectData) 
  show_data = show_data[1:min(max_data_report,nrow(show_data)),]
  row<-rownames(show_data)
  dfnew<-cbind(row,show_data)
  change<-colnames(dfnew)
  change[1]<-"Observation"
  colnames (dfnew)<-change
  m1<-gvisTable(dfnew,options=list(showRowNumber=TRUE,width=1220, height=min(400,27*(nrow(show_data)+1)),allowHTML=TRUE,page='disable'))
  print(m1,'chart')
  
  } else {
    print(xtable(NEW_ProjectData, caption="Only 1 derived variable generated", digits=3), type="html",html.table.attributes="class='table table-striped table-hover table-bordered'", caption.placement="top", comment=FALSE, include.rownames=TRUE, include.colnames=TRUE)
    }
```
</div>
</div>
<br> <br>

We now can replace our original data with the new one and continue our analysis. We can now visualize our original data using only the newly derived attributes. Here is the plot when we use only the top 2 factors:


```{r Fig2, echo=FALSE, comment=NA, results='asis', message=FALSE, echo=FALSE, fig.align='center', fig=TRUE}

if(ncol(NEW_ProjectData) >= 2) {
  df1  <- cbind(NEW_ProjectData[, 1], NEW_ProjectData[, 2])
  df1  <- as.data.frame(df1)
  sca1 <- gvisScatterChart(df1, options=list(legend="none",
                                             lineWidth=0, pointSize=8, hAxis.title="Derived Variable (Factor) 2",
                                             title="Data Visualization Using the top 2 Derived Attributes (Factors)", vAxis="{title:'Derived Variable (Factor) 1'}",
                                             hAxis="{title:'Derived Variable (Factor) 2'}", width=900, height=800))
  print(sca1,'chart')
  } else {
    df2  <- cbind(NEW_ProjectData[, 1], ProjectData[, 1])
    df2  <- as.data.frame(df2)
    sca2 <- gvisScatterChart(df2, options=list(legend="none",
                                               lineWidth=0, pointSize=12, hAxis.title="Derived Variable (Factor) 1",
                                               title="Only 1 Derived Variable: Using Initial Variable", vAxis="{title:'Derived Variable (Factor) 1'}",
                                               hAxis="{title:'Initial Variable (Factor) 1'}", width=900, height=800))
    print(sca2,'chart')
    }

```


```{r echo=FALSE, message=FALSE, prompt=FALSE, results='asis'}
# let's make the data into data.matrix classes so that we can easier visualize them
ProjectData = data.matrix(ProjectData)
```

<br>


Our Approach Part #2
---------------------------------------------------------------

#### Step 1. Decide which variables to use for clustering

Then, we decided to regroup product categories in subgroup in order to highlight some corrections and trends across different categories, which at first sight may not be correlated. However, we initially and intuitively believed that some correlation between some categories may exist, justifying why we decided to run this study. For instance, in hot season (march and april), stores usually sell a lot of beverages and fans but not that many in cool season season (November to january).

#### Step 2. Define similarity or dissimilarity measures between observations

In our case we explore two distance metrics: the commonly used **Euclidean distance** as well as a simple one we define manually. 

The Euclidean distance between two observations (in our case, customers) is simply the square root of the average of the square difference between the attributes of the two observations (in our case, customers). For example, the distance of the first customer in our data from customers 2-5 (summarized above), using their responses to the 6 attitudinal questions is:

```{r include=FALSE, echo=FALSE, comment=NA, warning=FALSE, message=FALSE}
euclidean_pairwise <- as.matrix(dist(head(ProjectData_segment, 5), method="euclidean"))
euclidean_pairwise <- euclidean_pairwise*lower.tri(euclidean_pairwise) + euclidean_pairwise*diag(euclidean_pairwise) + 10e10*upper.tri(euclidean_pairwise)
euclidean_pairwise[euclidean_pairwise==10e10] <- NA
```

<div class="row">
<div class="col-md-4">
```{r echo=FALSE, comment=NA, warning=FALSE, message=FALSE, fig.align='center', results='asis'}
print(xtable(euclidean_pairwise, caption="Pairwise Distances between the first 5 observations using The Euclidean Distance Metric", digits=1), type="html", html.table.attributes = "class='table table-striped table-hover table-bordered'", caption.placement="top", comment = FALSE, include.rownames = FALSE)
```
</div>
</div>

Notice for example that if we use, say, the Manhattan distance metric, these distances change as follows:

```{r include=FALSE, echo=FALSE, comment=NA, warning=FALSE, message=FALSE}
manhattan_pairwise <- as.matrix(dist(head(ProjectData_segment, 5), method="manhattan"))
manhattan_pairwise <- manhattan_pairwise*lower.tri(manhattan_pairwise) + manhattan_pairwise*diag(manhattan_pairwise) + 10e10*upper.tri(manhattan_pairwise)
manhattan_pairwise[manhattan_pairwise==10e10] <- NA
```

<div class="row">
<div class="col-md-4">
```{r echo=FALSE, comment=NA, warning=FALSE, message=FALSE, fig.align='center', results='asis'}
print(xtable(manhattan_pairwise, caption="Pairwise Distances between the first 5 observations using The Manhattan Distance Metric", digits=1), type="html", html.table.attributes = "class='table table-striped table-hover table-bordered'", caption.placement="top", comment = FALSE, include.rownames = FALSE)
```
</div>
</div>

Let's now define our own distance metric, it is easy to write this distance function in R:

```{r ,results='asis'}
My_Distance_function<-function(x,y){sum(abs(x-y)>2)}

```


#### Step 3. Visualize Individual Attributes and  Pair-wise Distances between the Observations


In our case we can see the histogram of, say, the first 4 variables:

```{r echo=FALSE, comment=NA, warning=FALSE, message=FALSE, echo=FALSE, error=FALSE, fig.align='center', results='asis'}

if (0){
  
  ProjectDataframe = data.frame(ProjectData_segment[,1:min(4,ncol(ProjectData_segment))])
  V1 <- ggplot() + geom_bar(aes(y = ..count.., x = V1),data=ProjectDataframe) + ylab(label = 'Frequency') + xlab(label = 'Histogram of Variable 1') + theme_grey()
  V2 <- ggplot() + geom_bar(aes (y = ..count.., x = V2),data=ProjectDataframe) + ylab(label = 'Frequency') + xlab(label = 'Histogram of Variable 2') + theme_grey()
  V3 <- ggplot() + geom_bar(aes (y = ..count.., x = V3),data=ProjectDataframe) + ylab(label = 'Frequency') + xlab(label = 'Histogram of Variable 3') + theme_grey()
  V4 <- ggplot() + geom_bar(aes (y = ..count.., x = V4),data=ProjectDataframe) + ylab(label = 'Frequency') + xlab(label = 'Histogram of Variable 4') + theme_grey()
  grid.arrange(V1, V2, V3, V4)
  
  }

```

or the histogram of all pairwise distances for the `r distance_used` distance:


```{r echo=FALSE, comment=NA, warning=FALSE, message=FALSE, fig.align='center', results='asis'}
Pairwise_Distances <- dist(ProjectData_segment, method = distance_used) 
hist(Pairwise_Distances, main = NULL, xlab="Histogram of all pairwise Distances between observtions", ylab="Frequency")
```

<blockquote> <p>
Visualization is very important for data analytics, as it can provide a first understanding of the data.
</p> </blockquote>

<br> 

#### Step 4. Select the clustering method to use and decide how many clusters to have

In this case the dendrogram, using the `r distance_used` distance metric from the earlier steps and the ``r hclust_method` hierarchical clustering option (see below as well as help(hclust) in R for more information), is as follows:

```{r echo=FALSE, comment=NA, warning=FALSE, message=FALSE, fig.align='center', results='asis'}
Hierarchical_Cluster_distances <- dist(ProjectData_segment, method=distance_used)
Hierarchical_Cluster <- hclust(Hierarchical_Cluster_distances, method=hclust_method)
# Display dendogram
plot(Hierarchical_Cluster, main = NULL, sub=NULL, labels = 1:nrow(ProjectData_segment), xlab="Our Observations", cex.lab=1, cex.axis=1) 
# Draw dendogram with red borders around the 3 clusters
rect.hclust(Hierarchical_Cluster, k=numb_clusters_used, border="red") 
```


Hence, we applied the 'clusterization' process in order to group product categories by clusters based on sales correlation across the 4 periods (4 segments mentioned above).  We gets 8 clusters with different trends mostly explained by top items in each cluster: 


```{r echo=FALSE, comment=NA, warning=FALSE, message=FALSE, fig.align='center', results='asis'}
max <- nrow(ProjectData)
num <- max - 1
df1 <- cbind(as.data.frame(Hierarchical_Cluster$height[length(Hierarchical_Cluster$height):1]), c(1:num))
colnames(df1) <- c("distances","index")
Line <- gvisLineChart(as.data.frame(df1), xvar="index", yvar="distances", options=list(title='Distances plot', legend="right", width=900, height=600, hAxis="{title:'Number of Components', titleTextStyle:{color:'black'}}", vAxes="[{title:'Distances'}]", series="[{color:'green',pointSize:3, targetAxisIndex: 0}]"))
print(Line,'chart')
```

For now let's consider the `r numb_clusters_used`-segments solution found by the Hierarchical Clustering method (using the `r distance_used` distance and the hclust option `r hclust_method`). We can also see the segment each observation (respondent in this case) belongs to for the first `r min(max_data_report,nrow(ProjectData))` people:


```{r echo=FALSE, comment=NA, warning=FALSE, message=FALSE,results='asis'}
cluster_memberships_hclust <- as.vector(cutree(Hierarchical_Cluster, k=numb_clusters_used)) # cut tree into 3 clusters
cluster_ids_hclust=unique(cluster_memberships_hclust)

ProjectData_with_hclust_membership <- cbind(1:length(cluster_memberships_hclust),cluster_memberships_hclust)
colnames(ProjectData_with_hclust_membership)<-c("Observation Number","Cluster_Membership")
```


<div class="row">
<div class="col-md-6">
```{r echo=FALSE, message=FALSE, prompt=FALSE, results='asis'}
show_data = data.frame(round(ProjectData_with_hclust_membership,2))
show_data = show_data[1:min(max_data_report,nrow(show_data)),]
row<-rownames(show_data)
dfnew<-cbind(row,show_data)
change<-colnames(dfnew)
change[1]<-"Variables"
colnames (dfnew)<-change
m1<-gvisTable(dfnew,options=list(showRowNumber=TRUE,width=1220, height=min(400,27*(nrow(show_data)+1)),allowHTML=TRUE,page='disable'))
print(m1,'chart')
```
</div>
</div>
<br> <br>


#### Step 7. Profile and interpret the clusters 

**Cluster 1**: Hot sales products  (rice, palm oil, coffee) with sales highly dependent on promotion (year end)<br>
**Cluster 2&4**: Mainly TV and DVD player, sales are relatively stable sales across the year.<br>
**Cluster 3**: High sales in february (chinese new year), april and may (peak season) but low in december. Mostly sugar which is a strictly regulated commodity with quota in Thailand. Quota sometimes allocated/depleted before december.<br>
**Cluster 5**: Key items in this cluster is Beer and Fan. Peak sales in April&may during warm season. High sales for beer in year due to massive year-end promotion.<br>
**Cluster 6**: Sales with moderate volatility, mostly IT hardware except in april: back to school season, students buy uniform and renew laptop and printer.<br>
**Cluster 7**: Sales relatively stable across the year except massive peak of sales in Nov & Dec. Top 10 of the products is Alcohol & White goods (washing machine and refrigerator) that correspond to Samsung and other Appliance supplier promotion at year end to reach yearly sales objective. <br>
**Cluster 8**: Sales with high volatility with peak of sales in Nov & Dec. Top 10 of the products is Oral Care/ Body Hygiene that correspond to massive P&G/unilever promotion to close year sales objective. We also find Thai Gift basket which is a product only sold in December as per Thai tradition.

The average values of our data for the total population as well as within each customer segment are:


```{r echo=FALSE, comment=NA, warning=FALSE, message=FALSE,results='asis'}
cluster_memberships <- cluster_memberships_hclust
cluster_ids <-  cluster_ids_hclust  
if (profile_with == "hclust"){
  cluster_memberships <- cluster_memberships_hclust
  cluster_ids <-  cluster_ids_hclust  
  }
if (profile_with == "kmeans"){
  cluster_memberships <- cluster_memberships_kmeans
  cluster_ids <-  cluster_ids_kmeans
  }

# SAVE THE DATA in the cluster file
NewData = matrix(cluster_memberships,ncol=1)
write.csv(NewData,file=cluster_file)

population_average = matrix(apply(ProjectData_profile, 2, mean), ncol=1)
colnames(population_average) <- "Population"
Cluster_Profile_mean <- sapply(sort(cluster_ids), function(i) apply(ProjectData_profile[(cluster_memberships==i), ], 2, mean))
if (ncol(ProjectData_profile) <2)
  Cluster_Profile_mean=t(Cluster_Profile_mean)
colnames(Cluster_Profile_mean) <- paste("Segment", 1:length(cluster_ids), sep=" ")
cluster.profile <- cbind (population_average,Cluster_Profile_mean)
```


<div class="row">
<div class="col-md-6">
```{r echo=FALSE, comment=NA, warning=FALSE, message=FALSE, results='asis'}
show_data = data.frame(round(cluster.profile,2))
#show_data = show_data[1:min(max_data_report,nrow(show_data)),]
row<-rownames(show_data)
dfnew<-cbind(row,show_data)
change<-colnames(dfnew)
change[1]<-"Variables"
colnames (dfnew)<-change
m1<-gvisTable(dfnew,options=list(showRowNumber=TRUE,width=1220, height=min(400,27*(nrow(show_data)+1)),allowHTML=TRUE,page='disable'))
print(m1,'chart')

```
</div>
</div>

We can also compare the averages of the profiling variables of each segment relative to the average of the variables across the whole population. This can also help us better understand whether there are indeed clusters in our data (e.g. if all segments are much like the overall population, there may be no segments). For example, we can measure the ratios of the average for each cluster to the average of the population (e.g. avg(cluster)/avg(population)) and explore a matrix as the following one:

```{r echo=FALSE, comment=NA, warning=FALSE, message=FALSE, results='asis'}
population_average_matrix <- population_average[,"Population",drop=F] %*% matrix(rep(1,ncol(Cluster_Profile_mean)),nrow=1)
cluster_profile_ratios <- (ifelse(population_average_matrix==0, 0,Cluster_Profile_mean/population_average_matrix))
colnames(cluster_profile_ratios) <- paste("Segment", 1:ncol(cluster_profile_ratios), sep=" ")
rownames(cluster_profile_ratios) <- colnames(ProjectData)[profile_attributes_used]

## printing the result in a clean-slate table
cat(renderHeatmapX(cluster_profile_ratios, border=1, center = 1, minvalue = heatmin))
```


Intepretation, Decisions and Conclusion
----------------------------------------------------------------

As a conclusion, our model **simplified the timeline** by introducing four seasons in the year (instead of 12 months) and provided a **simple sales forecast** - relative to a sales baseline - for each cluster of products, each cluster grouping many different product categories.

With additional further developments to refine these results, this model may have **multiple applications**. For instance, it may help to better understand existing sales trend for existing assortment and better manage sales plan, reduce leftover while strictly preventing out of stock at store level. It may also be useful when introducing new products in the assortment. Thus, one way to proceed would be to anticipate sales variation across the four seasons by identifying the cluster of products where the new product belong and apply the trend to a sales baseline.
